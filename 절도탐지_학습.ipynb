{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I5QEEJijtWF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import cv2\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "import csv\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQhQNET6Nvkf",
        "outputId": "00f23998-6918-483e-8515-291670eb98f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuTNThq_FWds"
      },
      "outputs": [],
      "source": [
        "data_root = '/content/drive/MyDrive/ai기반 ocr/컴퓨터비전/이상행동감지'\n",
        "file_root = f'{data_root}/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJsQxDSWFtxw",
        "outputId": "1679a66b-7795-4ca3-cab5-c1b9c41732e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['C_3_12_40_BU_SMC_10-14_11-43-44_CB_RGB_DF2_M2.mp4',\n",
              " 'C_3_12_40_BU_SMC_10-14_11-43-44_CA_RGB_DF2_M2.mp4',\n",
              " 'C_3_12_40_BU_DYB_10-16_14-49-01_CD_RGB_DF2_M1.mp4',\n",
              " 'C_3_12_40_BU_DYB_10-16_14-49-01_CB_RGB_DF2_M1.mp4',\n",
              " 'C_3_12_40_BU_SMC_10-14_11-43-44_CC_RGB_DF2_M2.mp4',\n",
              " 'C_3_12_40_BU_DYA_07-29_16-07-39_CE_RGB_DF2_F2.mp4',\n",
              " 'C_3_12_40_BU_DYA_07-29_16-07-39_CD_RGB_DF2_F2.mp4',\n",
              " 'C_3_12_39_BU_SMC_10-14_11-41-49_CE_RGB_DF2_M2.mp4',\n",
              " 'C_3_12_40_BU_DYB_10-16_14-49-01_CA_RGB_DF2_M1.mp4',\n",
              " 'C_3_12_40_BU_DYA_07-29_16-07-39_CF_RGB_DF2_F2.mp4']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "file_list = os.listdir(f'{file_root}/image_id')\n",
        "file_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vBDuj1WgIrI-"
      },
      "outputs": [],
      "source": [
        "# mp4에서 이미지 추출하기\n",
        "# 각 mp4 이름으로 디렉토리 생성 후 저장\n",
        "\n",
        "# for file in tqdm(file_list):\n",
        "#   name = file.split('.')[0]\n",
        "#   if not os.path.isdir(f'{file_root}/images/{name}'):\n",
        "#     os.mkdir(f'{file_root}/images/{name}')\n",
        "#   video_path = f'{file_root}/image_id/{file}'\n",
        "#   print(video_path)\n",
        "#   cap = cv2.VideoCapture(video_path)\n",
        "#   num = 1\n",
        "#   while(cap.isOpened()):\n",
        "#     ret, frame = cap.read()\n",
        "#     if ret:\n",
        "#       cv2.imwrite(f'{file_root}/images/{name}/{name}_{num:06d}.png', frame)\n",
        "#       num += 1\n",
        "#     else:\n",
        "#       break\n",
        "#   cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4YuIrgHN-MF",
        "outputId": "fd524df1-557e-417a-a96b-552ea90d9fbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# 프레임이 있는 파일 리스트만 추출\n",
        "file_list = os.listdir(f'{file_root}/image_id')\n",
        "# len(file_list)\n",
        "real_file_list = []\n",
        "for file in file_list:\n",
        "  temp = os.listdir(f\"{file_root}/images/{file.split('.')[-2]}\")\n",
        "  if len(temp) > 0:\n",
        "    real_file_list.append(file)\n",
        "len(real_file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLdvXgovTLJZ",
        "outputId": "dfc700a0-cc4e-4183-8a1f-0bb172fd2ebe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['C_3_12_40_BU_SMC_10-14_11-43-44_CB_RGB_DF2_M2.mp4',\n",
              " 'C_3_12_40_BU_SMC_10-14_11-43-44_CA_RGB_DF2_M2.mp4',\n",
              " 'C_3_12_40_BU_DYB_10-16_14-49-01_CD_RGB_DF2_M1.mp4',\n",
              " 'C_3_12_40_BU_DYB_10-16_14-49-01_CB_RGB_DF2_M1.mp4',\n",
              " 'C_3_12_40_BU_SMC_10-14_11-43-44_CC_RGB_DF2_M2.mp4',\n",
              " 'C_3_12_40_BU_DYA_07-29_16-07-39_CE_RGB_DF2_F2.mp4',\n",
              " 'C_3_12_40_BU_DYA_07-29_16-07-39_CD_RGB_DF2_F2.mp4',\n",
              " 'C_3_12_39_BU_SMC_10-14_11-41-49_CE_RGB_DF2_M2.mp4',\n",
              " 'C_3_12_40_BU_DYB_10-16_14-49-01_CA_RGB_DF2_M1.mp4',\n",
              " 'C_3_12_40_BU_DYA_07-29_16-07-39_CF_RGB_DF2_F2.mp4']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "real_file_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BTixJHaPD4i"
      },
      "outputs": [],
      "source": [
        "# random.seed(2024)\n",
        "# random.shuffle(real_file_list)\n",
        "# test_ratio = 0.1\n",
        "\n",
        "# num_file = len(real_file_list)\n",
        "\n",
        "# test_list = real_file_list[:int(num_file*test_ratio)]\n",
        "# valid_list = real_file_list[int(num_file*test_ratio):int(num_file*test_ratio)*2]\n",
        "# train_list = real_file_list[int(num_file*test_ratio)*2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPiW7HNmPsHt"
      },
      "outputs": [],
      "source": [
        "def read_one_xml(xml_path):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    event_dict = {\n",
        "        'event': None,\n",
        "        'start_frame': -1,\n",
        "        'end_frame': -1\n",
        "    }\n",
        "    for child in root.iter('track'):\n",
        "        if child.attrib['label'] == 'theft_start':\n",
        "            event_dict['event'] = child.attrib['label'].split('_')[0]\n",
        "            frame = child.find('box').attrib['frame']\n",
        "            event_dict['start_frame'] = int(frame)\n",
        "        elif child.attrib['label'] == 'theft_end':\n",
        "            event_dict['event'] = child.attrib['label'].split('_')[0]\n",
        "            try:\n",
        "                frame = child.find('box').attrib['frame']\n",
        "            except:\n",
        "                return {}, {}\n",
        "            event_dict['end_frame'] = int(frame)\n",
        "    return event_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMuaMBIkP5mI"
      },
      "outputs": [],
      "source": [
        "# XML 파일에서 행동 정보 추출하기\n",
        "event_list = []\n",
        "# {'event': 'theft', 'start_frame': X, 'end_frame': X}\n",
        "for i in range(len(real_file_list)):\n",
        "  file = real_file_list[i]\n",
        "  xml_path = f\"{file_root}/labels_id/{file.replace('mp4', 'xml')}\"\n",
        "  event_dict = read_one_xml(xml_path)\n",
        "  event_list.append(event_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "event_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4132NcckkF_9",
        "outputId": "2fb5b895-6f72-42be-f56c-1c3ef2a1ef9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'event': 'theft', 'start_frame': 79, 'end_frame': 150},\n",
              " {'event': 'theft', 'start_frame': 79, 'end_frame': 151},\n",
              " {'event': 'theft', 'start_frame': 80, 'end_frame': 123},\n",
              " {'event': 'theft', 'start_frame': 81, 'end_frame': 126},\n",
              " {'event': 'theft', 'start_frame': 79, 'end_frame': 150},\n",
              " {'event': 'theft', 'start_frame': 121, 'end_frame': 145},\n",
              " {'event': 'theft', 'start_frame': 121, 'end_frame': 142},\n",
              " {'event': 'theft', 'start_frame': 82, 'end_frame': 145},\n",
              " {'event': 'theft', 'start_frame': 81, 'end_frame': 126},\n",
              " {'event': 'theft', 'start_frame': 123, 'end_frame': 148}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enfaREpNr0Qj"
      },
      "outputs": [],
      "source": [
        "def load_img(link):\n",
        "  # 1. 이미지 읽어들이기\n",
        "  image = Image.open(link)\n",
        "\n",
        "  # 2. 이미지 전처리 및 텐서로 변환\n",
        "  transform = transforms.Compose([\n",
        "      transforms.Resize((224, 224)),  # 이미지 크기 조정 (ResNet50이 요구하는 입력 크기)\n",
        "      transforms.ToTensor(),          # 이미지를 텐서로 변환 (픽셀 값을 [0, 1] 범위로 정규화)\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 이미지 정규화\n",
        "  ])\n",
        "\n",
        "  # 3. 변환 적용\n",
        "  tensor_image = transform(image)\n",
        "\n",
        "  # 4. 배치 차원 추가 (모델 입력을 위해 배치 차원이 필요함)\n",
        "  # tensor_image = tensor_image.unsqueeze(0)  # (C, H, W) -> (1, C, H, W)\n",
        "  # print(tensor_image.shape)  # 텐서 형태 확인\n",
        "  return tensor_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9I7KpiOsIGy"
      },
      "outputs": [],
      "source": [
        "frame_list = []\n",
        "label_list = []\n",
        "for i in range(len(real_file_list)-4):\n",
        "  file = real_file_list[i]\n",
        "  name = file.split('.')[0]\n",
        "  if event_list[i] is not None:\n",
        "      current_frame = 0\n",
        "      while(current_frame < event_list[i]['start_frame']+4):\n",
        "          if (current_frame+3) >= event_list[i]['start_frame']:\n",
        "              event_num = 1\n",
        "          else:\n",
        "              event_num = 0\n",
        "          data = [load_img(f'/content/drive/MyDrive/ai기반 ocr/컴퓨터비전/이상행동감지/data/images/{name}/{name}_{(current_frame+1):06d}.png'),\n",
        "              load_img(f'/content/drive/MyDrive/ai기반 ocr/컴퓨터비전/이상행동감지/data/images/{name}/{name}_{(current_frame+2):06d}.png'),\n",
        "              load_img(f'/content/drive/MyDrive/ai기반 ocr/컴퓨터비전/이상행동감지/data/images/{name}/{name}_{(current_frame+3):06d}.png')]\n",
        "          data = torch.stack(data)\n",
        "          frame_list.append(data)\n",
        "          label_list.append(event_num)\n",
        "          current_frame += 1\n",
        "      data = [load_img(f'/content/drive/MyDrive/ai기반 ocr/컴퓨터비전/이상행동감지/data/images/{name}/{name}_{(current_frame+1):06d}.png'),\n",
        "              load_img(f'/content/drive/MyDrive/ai기반 ocr/컴퓨터비전/이상행동감지/data/images/{name}/{name}_{(current_frame+2):06d}.png'),\n",
        "              load_img(f'/content/drive/MyDrive/ai기반 ocr/컴퓨터비전/이상행동감지/data/images/{name}/{name}_{(current_frame+3):06d}.png')]\n",
        "      data = torch.stack(data)\n",
        "      frame_list.append(data)\n",
        "      label_list.append(event_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeIvI3fzjGk4",
        "outputId": "07a373d4-47bf-4961-f4cd-bd3a12a5dce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame has been saved to ./train.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Column1': frame_list,\n",
        "    'Column2': label_list\n",
        "})\n",
        "\n",
        "# 3. 데이터 프레임을 CSV 파일로 저장\n",
        "csv_file_path = './train.csv'  # 저장할 파일 경로와 이름\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "print(f\"DataFrame has been saved to {csv_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1pbDBJqutQU"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_label_1 = df[df['Column2'] == 1]\n",
        "df_label_0 = df[df['Column2'] == 0].sample(n=len(df_label_1), random_state=42)\n",
        "df_balanced = pd.concat([df_label_1, df_label_0])\n",
        "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "ycyUwSS8kQjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = './diff.csv'  # 저장할 파일 경로와 이름\n",
        "df_balanced.to_csv(csv_file_path, index=False)"
      ],
      "metadata": {
        "id": "Io3n62XUpttT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# frame_list = df['Column1'].to_list()\n",
        "# label_list = df['Column2'].to_list()\n",
        "\n",
        "frame_list = df_balanced['Column1'].to_list()\n",
        "label_list = df_balanced['Column2'].to_list()"
      ],
      "metadata": {
        "id": "2GOdYC-bkTQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsasb8WYwaXX"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "data = torch.stack(frame_list)\n",
        "label = torch.tensor(label_list)\n",
        "dataset = TensorDataset(data, label)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh1kyH6ilUm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7743431-9081-4dfe-cedc-09327156e17e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 113MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "class ResNetConv3DLSTM(nn.Module):\n",
        "    def __init__(self, num_classes=1):  # 이진 분류이므로 num_classes=1\n",
        "        super(ResNetConv3DLSTM, self).__init__()\n",
        "\n",
        "        # ResNet50 backbone\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet = nn.Sequential(*list(resnet.children())[:-2])  # Fully connected layer 제거\n",
        "        self.resnet_out_channels = 2048\n",
        "\n",
        "        # Conv3D layer\n",
        "        self.conv3d = nn.Conv3d(in_channels=self.resnet_out_channels, out_channels=512, kernel_size=(3, 3, 3), stride=1, padding=1)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_frames, C, H, W = x.shape\n",
        "\n",
        "        # ResNet feature extraction\n",
        "        x = x.view(batch_size * num_frames, C, H, W)  # ResNet 입력 준비\n",
        "        x = self.resnet(x)  # ResNet 출력: [batch_size * num_frames, 2048, H', W']\n",
        "        x = x.view(batch_size, num_frames, self.resnet_out_channels, x.size(2), x.size(3))  # [batch_size, num_frames, 2048, H', W']\n",
        "        x = x.permute(0, 2, 1, 3, 4)  # Conv3D 입력: [batch_size, 2048, num_frames, H', W']\n",
        "\n",
        "        # Conv3D\n",
        "        x = self.conv3d(x)  # Conv3D 출력: [batch_size, 512, num_frames, H', W']\n",
        "        x = torch.mean(x, dim=[3, 4])  # Global average pooling: [batch_size, 512, num_frames]\n",
        "\n",
        "        # LSTM\n",
        "        x = x.permute(0, 2, 1)  # LSTM 입력: [batch_size, num_frames, 512]\n",
        "        x, _ = self.lstm(x)  # LSTM 출력: [batch_size, num_frames, 256]\n",
        "        x = x[:, -1, :]  # 마지막 타임스텝의 hidden state: [batch_size, 256]\n",
        "\n",
        "        # Fully connected\n",
        "        x = self.fc(x)  # 출력: [batch_size, 1]\n",
        "\n",
        "        # Sigmoid activation 추가\n",
        "        x = torch.sigmoid(x)  # 출력값을 [0, 1] 범위로 변환\n",
        "\n",
        "        return x.squeeze(1)  # [batch_size]로 크기 맞추기 위해 squeeze\n",
        "\n",
        "model = ResNetConv3DLSTM(num_classes=1)  # 이진 분류이므로 num_classes=1\n",
        "model = model.to(device)\n",
        "criterion = nn.BCELoss()  # 이진 분류를 위한 손실 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLKp9oc5o5Y2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Initialize optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = torch.nn.BCELoss()  # Assume binary classification\n",
        "\n",
        "# 예제 학습 루프\n",
        "num_epochs = 100\n",
        "best_accuracy = 0.0\n",
        "best_model_weights = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in dataloader:  # Assume dataloader is defined\n",
        "        inputs_, labels_ = inputs.to(device), labels.to(device)\n",
        "        labels_ = labels_.float()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs_)\n",
        "        loss = criterion(outputs, labels_)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # 정확도 계산\n",
        "        predicted_labels = (outputs >= 0.5).float()  # 0.5를 기준으로 이진 레이블로 변환\n",
        "        correct_predictions += (predicted_labels == labels_).sum().item()\n",
        "        total_samples += labels_.size(0)\n",
        "\n",
        "    # Epoch당 평균 손실과 정확도 계산\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_accuracy = correct_predictions / total_samples\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.8f}, Accuracy: {epoch_accuracy:.8f}\")\n",
        "\n",
        "    # 최고 정확도일 경우 모델 가중치 저장\n",
        "    if epoch_accuracy > best_accuracy:\n",
        "        best_accuracy = epoch_accuracy\n",
        "        best_model_weights = model.state_dict()\n",
        "\n",
        "# 최종적으로 최고 정확도를 기록한 모델 가중치 저장\n",
        "if best_model_weights is not None:\n",
        "    torch.save(best_model_weights, 'best_model_weights2.pth')\n",
        "    print(f\"Best model saved with accuracy: {best_accuracy:.8f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rnHSuiIRPoC4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}